{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation on Aerial Images\n",
    "\n",
    "I present here my best model for the task. In a nutshell it is a U-Net architecture with pre-trained Encoder and a fairly standard Decoder stage and some custom losses.\n",
    "\n",
    "First of all I install from a github repository the model used for the encoding part of the network, an EfficientNet B7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!pip install -U git+https://github.com/qubvel/efficientnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that there's the standard input pipeline consisting in: \n",
    "* setting the seeds\n",
    "* checking for the GPU\n",
    "* import of the data with a training/validation set splitting and creation of the dataset. \n",
    "\n",
    "Since now I'm using a pretrained network I give as input an additional parameter `preprocess_input` which is the correct function to use for image pre-processing for the network we're going to use in the training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from efficientnet.tfkeras import preprocess_input\n",
    "import tensorflow as tf \n",
    "import data_utils as utils\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "SEED = 42\n",
    "utils.set_seed(SEED)\n",
    "    \n",
    "utils.check_gpu()\n",
    "\n",
    "BS = 8\n",
    "val_split = 0.15\n",
    "apply_data_augmentation = False\n",
    "preprocess_input = preprocess_input \n",
    "train_dataset, train_img_gen, valid_dataset, valid_img_gen = utils.train_val_dataset(val_split, apply_data_augmentation,BS,SEED,preprocess_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation \n",
    "\n",
    "Now let's analyze the creation of the model. As explained earlier, I used EfficientNet B7 as a backbone. Due to the rather small number of parameters I've tried to re-train it completely rather than freeze the weights in order to achieve better performances. \n",
    "\n",
    "So I set the whole model to be `trainable` and then I extracted: \n",
    "* the output of the last layer in order to attach the decoder\n",
    "* the skip connection points from the network \n",
    "\n",
    "This network is very complicated but I report here the points I used to do skip connections between encoder and decoder\n",
    "\n",
    "```python\n",
    "std_skip_connections = [\n",
    "    'block2a_expand_activation',\n",
    "    'block3a_expand_activation',\n",
    "    'block4a_expand_activation',\n",
    "    'block6a_expand_activation',\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "For the decoder I opted for a 5-layers-deep network based on the following building block:\n",
    "* **UpSampling**\n",
    "* **Concatenation** (skip connection)\n",
    "* **Conv2D layer**\n",
    "* **Batch Normalization**\n",
    "* **Activation Layer** with **ReLU** activation function\n",
    "* **Conv2D layer**\n",
    "* **Batch Normalization**\n",
    "* **Activation Layer** with **ReLU** activation function\n",
    "\n",
    "The number of filters of the two `Conv2D` in the convolution block of the decode from 256 (at the very bottom) to 16 as powers of 2. \n",
    "\n",
    "The last layer is standard, a single `1x1` filters with a `sigmoid` activation function. \n",
    "\n",
    "---\n",
    "\n",
    "Notice that I also insert some `Dropout` layers in the Decoder to avoid overfitting. In particular I choose the maxmun amount of dropout (`0.5`) in the deepest (with more filters) layers and as we go shallower the dropout factor reduce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from efficientnet.tfkeras import EfficientNetB7\n",
    "\n",
    "DIM = 256\n",
    "batchnorm = True\n",
    "\n",
    "# ENCODER\n",
    "base = EfficientNetB7(weights='imagenet', include_top=False, input_shape=(DIM, DIM, 3),classes=1)\n",
    "base.trainable = True\n",
    "base_out = base.output\n",
    "\n",
    "std_skip_connections = [\n",
    "    'block2a_expand_activation',\n",
    "    'block3a_expand_activation',\n",
    "    'block4a_expand_activation',\n",
    "    'block6a_expand_activation',\n",
    "]\n",
    "\n",
    "def conv2d_block(input_tensor, n_filters, kernel_size=3, batchnorm=True, seed=42):\n",
    "    \n",
    "    ker_init = tf.keras.initializers.he_normal(seed=seed)\n",
    "    \n",
    "        # LAYER 1\n",
    "    x = tf.keras.layers.Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=ker_init, padding=\"same\")(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "        \n",
    "        # LAYER 2\n",
    "    x = tf.keras.layers.Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=ker_init, padding=\"same\")(x)\n",
    "    if batchnorm:\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "u6 = tf.keras.layers.UpSampling2D(size=(2, 2), data_format=None, interpolation='bilinear') (base_out)\n",
    "u6 = tf.keras.layers.concatenate([u6, base.get_layer(std_skip_connections[-1]).output], axis=-1)\n",
    "u6 = tf.keras.layers.Dropout(0.5)(u6)\n",
    "c6 = conv2d_block(u6, n_filters=256, kernel_size=3, batchnorm=batchnorm, seed=SEED)\n",
    "\n",
    "u7 = tf.keras.layers.UpSampling2D(size=(2, 2), data_format=None, interpolation='bilinear') (c6)\n",
    "u7 = tf.keras.layers.concatenate([u7, base.get_layer(std_skip_connections[2]).output], axis=-1)\n",
    "u7 = tf.keras.layers.Dropout(0.5)(u7)\n",
    "c7 = conv2d_block(u7, n_filters=128, kernel_size=3, batchnorm=batchnorm, seed=SEED)\n",
    "\n",
    "u8 = tf.keras.layers.UpSampling2D(size=(2, 2), data_format=None, interpolation='bilinear') (c7)\n",
    "u8 = tf.keras.layers.concatenate([u8, base.get_layer(std_skip_connections[1]).output], axis=-1)\n",
    "u8 = tf.keras.layers.Dropout(0.5)(u8)\n",
    "c8 = conv2d_block(u8, n_filters=64, kernel_size=3, batchnorm=batchnorm, seed=SEED)\n",
    "\n",
    "u9 = tf.keras.layers.UpSampling2D(size=(2, 2), data_format=None, interpolation='bilinear') (c8)\n",
    "u9 = tf.keras.layers.concatenate([u9, base.get_layer(std_skip_connections[0]).output], axis=-1)\n",
    "u9 = tf.keras.layers.Dropout(0.25)(u9)\n",
    "c9 = conv2d_block(u9, n_filters=32, kernel_size=3, batchnorm=batchnorm, seed=SEED)\n",
    "\n",
    "u10 = tf.keras.layers.UpSampling2D(size=(2, 2), data_format=None, interpolation='bilinear') (c9)\n",
    "u10 = tf.keras.layers.Dropout(0.25)(u10)\n",
    "c10 = conv2d_block(u10, n_filters=16, kernel_size=3, batchnorm=batchnorm, seed=SEED)\n",
    "\n",
    "output_ = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid') (c10)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[base.input], outputs=[output_])\n",
    "                                  \n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function Definition \n",
    "\n",
    "For the loss function I opted for using different losses summed together. The underlying idea is that every one of them will help the model to learn more by focusing in different aspects:\n",
    "\n",
    "* **Dice Loss**: measure the similaity between the prediction and the ground thruth (`gt`)\n",
    "* **Focal Loss**: alternative to `binary_crossentropy` that strongly penalizes misclassification since it increases the weights for badly-classified pixels and decreases the weights for well-classified pixels. \n",
    "* **Boundary Loss**: Loss function that weights the border of the buildings forcing the network to be very accurate in the edge detection. \n",
    "\n",
    "---\n",
    "\n",
    "The boundary detection loss has been implemented inspiring to this [post](https://www.groundai.com/project/boundary-loss-for-remote-sensing-imagery-semantic-segmentation/1#S4.F1). It uses a pixel-wise pooling with a sliding window of size (3,3) on the inverted mask to gather the boundaries and with another pooling operation it weights each pixel with regards to the presence of multiple boundaries in a (5,5) neighborhood.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as backend\n",
    "from tensorflow_core.keras.backend import pool2d as pool\n",
    "\n",
    "SMOOTH = 1e-5\n",
    "\n",
    "def get_reduce_axes(per_image=None):\n",
    "    axes = [1, 2] if backend.image_data_format() == 'channels_last' else [2, 3]\n",
    "    if not per_image:\n",
    "        axes.insert(0, 0)\n",
    "    return axes\n",
    "\n",
    "def average(x, per_image=False):\n",
    "    if per_image:\n",
    "        x = backend.mean(x, axis=0)\n",
    "    return backend.mean(x)\n",
    "\n",
    "def f_score(gt, pr, beta=1, smooth=SMOOTH):\n",
    "\n",
    "    axes = get_reduce_axes()\n",
    "\n",
    "    tp = backend.sum(gt * pr, axis=axes)\n",
    "    fp = backend.sum(pr, axis=axes) - tp\n",
    "    fn = backend.sum(gt, axis=axes) - tp\n",
    "\n",
    "    score = ((1 + beta ** 2) * tp + smooth) / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + smooth)\n",
    "    score = average(score)\n",
    "    return score\n",
    "\n",
    "def binary_focal_loss(gt, pr, gamma=2.0, alpha=0.25):\n",
    "    # clip to prevent NaNs\n",
    "    pr = backend.clip(pr, backend.epsilon(), 1.0 - backend.epsilon())\n",
    "\n",
    "    loss_1 = - gt * (alpha * backend.pow((1 - pr), gamma) * backend.log(pr))\n",
    "    loss_0 = - (1 - gt) * ((1 - alpha) * backend.pow((pr), gamma) * backend.log(1 - pr))\n",
    "    loss = backend.mean(loss_0 + loss_1)\n",
    "    return loss\n",
    "\n",
    "def bf1_score(gt, pr):\n",
    "    gt = backend.cast(gt, 'float32')\n",
    "    pr = backend.cast(pr, 'float32')\n",
    "\n",
    "    b_true = pool(1-gt, pool_size=(3,3), strides=(1,1), padding='same') - (1-gt)\n",
    "    b_pred = pool(1-pr, pool_size=(3,3), strides=(1,1), padding='same') - (1-pr)\n",
    "\n",
    "    map_ext_true = pool(b_true, pool_size=(5,5), strides=(1,1), padding='same', pool_mode='avg')\n",
    "    map_ext_pred = pool(b_pred, pool_size=(5,5), strides=(1,1), padding='same', pool_mode='avg')\n",
    "\n",
    "    P = tf.reduce_sum(b_pred*map_ext_true)/tf.reduce_sum(b_pred)\n",
    "    R = tf.reduce_sum(b_true*map_ext_pred)/tf.reduce_sum(b_true)\n",
    "    return (2 * P * R)/(P + R)\n",
    "    \n",
    "def total_loss(y_true, y_pred):\n",
    "    bf1_loss = 1 - bf1_score(y_true, y_pred)\n",
    "    foc_loss = binary_focal_loss(y_true, y_pred)\n",
    "    dic_loss = 1 - f_score(y_true, y_pred)\n",
    "    return dic_loss + foc_loss + bf1_lossbf1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training \n",
    "\n",
    "The training procedure is standard. I defined the `IoU` metrics and compiled the model with the previously defined loss function and the `Adam` optimizer with an initial learning rate of `5e-5`. \n",
    "\n",
    "For the fitting procedure I used three different callbacks:\n",
    "* **`ReduceLROnPlateau`**: that reduce the learning rate of `factor` after `patience` epochs of non-descending `val_loss`. \n",
    "* **`EarlyStopping`**: in order to stop the fit if the `val_loss` does not improve after `patience` epochs. \n",
    "* **`ModelCheckpoint`**: to save the best model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as backend\n",
    "from tensorflow_core.keras.backend import pool2d as pool\n",
    "\n",
    "def IoU(y_true, y_pred):\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32) \n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "\n",
    "    return intersection / union\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=5e-5),\n",
    "    loss=total_loss,\n",
    "    metrics=[IoU])\n",
    "\n",
    "callback = []\n",
    "callback.append(tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.7,\n",
    "    patience=3))\n",
    "callback.append(tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10,\n",
    "    restore_best_weights=True))\n",
    "callback.append(tf.keras.callbacks.ModelCheckpoint(\n",
    "    './best_model.h5', \n",
    "    save_weights_only=True, \n",
    "    save_best_only=True, \n",
    "    mode='min'))\n",
    "\n",
    "model.fit(x=train_dataset,\n",
    "        epochs=150,\n",
    "        steps_per_epoch=len(train_img_gen),\n",
    "        validation_data=valid_dataset,\n",
    "        validation_steps=len(valid_img_gen),\n",
    "        callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizations and Creation of the CSV file \n",
    "\n",
    "After the training procedure I performed a cross-validation on the validation set for the threshold (for discernign 0s and 1s) in order to maximize the prediction accuracy. \n",
    "I did scan all the range between `0.05` and `0.95` and re-defined the metrics to take an additional parameter (the new threshold) as input. After that, in a `for` loop, I used the `model.evaluate()` method on the validation set and saved all the metrics performaces in a vector whose is then used for selecting the proper thershold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "n_cv = 10\n",
    "thresholds = np.linspace(0.25,0.75,n_cv)\n",
    "m = np.zeros((n_cv,1))\n",
    "\n",
    "def IoUscore(th):\n",
    "    def IoU(y_true, y_pred):\n",
    "        y_pred = tf.cast(y_pred > th, tf.float32) \n",
    "        intersection = tf.reduce_sum(y_true * y_pred)\n",
    "        union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "\n",
    "        return intersection / union\n",
    "    return IoU\n",
    "\n",
    "for i in range(n_cv):\n",
    "    metric = IoUscore(th=thresholds[i])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=5e-5),loss=total_loss,metrics=[metric])\n",
    "    m[i] = model.evaluate(valid_dataset, steps=len(valid_img_gen))[1]\n",
    "\n",
    "best_tm = thresholds[np.argmax(m)]\n",
    "print('OPT THRESHOLD: ',best_tm)\n",
    "print('BEST VAL: ', np.max(m))\n",
    "plt.figure()\n",
    "plt.plot(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point all that's left to do is the creation the output CSV file.\n",
    "During the various try of training I did notice that the masks' edges were rough and very noisy with some spots of misclassified pixels in the center of some roofs. \n",
    "In order to fix these problems I used some morphological transformation on the predicted (and thresholded) masks: \n",
    "* **closing**: is a dilation followed by erosion operation that helps closing small holes inside the foreground objects, or small black points on the object. \n",
    "* **opening**: is a erosion followed by dilation. It is useful in removing noise in the image. \n",
    "\n",
    "This procedure helpd a lot in improving the model accuracy.\n",
    "\n",
    "The `sub_csv_postprcess` function is very similar to the the `sub_csv` function that I used for making predictions and generating the CSV file in the other models, the only difference is that it emplyes the morphological transformation that I explained before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.sub_csv_postprocess(model,preprocess_input,best_tm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "The model is very long to train (on Kaggle platform, using GPU) taking the whole time window for GPU usage (9h). \n",
    "\n",
    "The model can be further improved using: \n",
    "* Data augmentation\n",
    "* Ensembles method (which I was not able to test because of the long training time and the limit amount of GPU in Kaggle)\n",
    "* Additional post-processing (e.g. polygonizing the predicted masks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
